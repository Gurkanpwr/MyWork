{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58636768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd44c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 32*8\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54194235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1637c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225573aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Adamw, SGD\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e417a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        \t\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \t\t\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382bc77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADAMW]:Accuracy of the network on the 10000 test images: 89.42 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('[ADAMW]:Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fcf728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a251e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a4ec3fe230>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].squeeze(), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65e7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Adamw, SGD\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3437b128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        \t\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \t\t\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9d6cd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADAM]:Accuracy of the network on the 10000 test images: 89.94 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('[ADAM]:Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "971a7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Adamw, SGD\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ac1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        \t\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \t\t\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1ba356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ADAMW]:Accuracy of the network on the 10000 test images: 89.7 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('[ADAMW]:Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b340462",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9)\n",
    "# Adamw, SGD\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d152c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        \t\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \t\t\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ebb3078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SGD]:Accuracy of the network on the 10000 test images: 85.72 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('[SGD]:Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea49e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
